{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "609006646970a83c89ff8a332459ddceb2e02e4c"
   },
   "source": [
    "# Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import gc\n",
    "import os     \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "#import lightgbm as lgb\n",
    "#from catboost import Pool, CatBoostClassifier\n",
    "import itertools\n",
    "import pickle, gzip\n",
    "import glob\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7dbccfa992f29644a47a31341b2c68c6b42b835d"
   },
   "source": [
    "# Extracting Features from train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "631c418da4275fba3070f2a6cff3873d11591f95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at passband 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noahkasmanoff/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at passband 1\n",
      "Looking at passband 2\n",
      "Looking at passband 3\n",
      "Looking at passband 4\n",
      "Looking at passband 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.enable()\n",
    "\n",
    "train = pd.read_csv('../data/training_set.csv')\n",
    "train['flux_ratio_sq'] = np.power(train['flux'] / train['flux_err'], 2.0)\n",
    "train['flux_by_flux_ratio_sq'] = train['flux'] * train['flux_ratio_sq']\n",
    "\n",
    "agg_db = pd.DataFrame([])#,columns='object_id')\n",
    "agg_db['object_id'] = train['object_id'].unique()\n",
    "\n",
    "#agg_db = pd.DataFrame(data=agg_db['object_id'].unique(),columns='object_id')\n",
    "for pband in sorted(train['passband'].unique()):\n",
    "    print(\"Looking at passband \" + str(pband))\n",
    "    pband_data = train.loc[train['passband'] == pband]\n",
    "    for col in pband_data.columns:\n",
    "        if col != 'object_id':\n",
    "            pband_data[col + str(pband)] = pband_data[col]\n",
    "            del pband_data[col]\n",
    "    #pband_data['passband # '+ str(pband)] = pband_data['passband']\n",
    "   # del pband_data['passband']\n",
    "\n",
    "    aggs = {\n",
    "        'mjd'+ str(pband): ['min', 'max', 'size'],\n",
    "        #'passband' + str(pband): ['min', 'max', 'mean', 'median', 'std'],\n",
    "        'flux'+ str(pband): ['min', 'max', 'mean', 'median', 'std','skew'],\n",
    "        'flux_err'+ str(pband): ['min', 'max', 'mean', 'median', 'std','skew'],\n",
    "        'detected'+ str(pband): ['mean'],\n",
    "        'flux_ratio_sq'+ str(pband):['sum','skew'],\n",
    "        'flux_by_flux_ratio_sq'+ str(pband):['sum','skew'],\n",
    "    }\n",
    "\n",
    "\n",
    "    agg_train = pband_data.groupby('object_id').agg(aggs)\n",
    "    new_columns = [\n",
    "        k + '_' + agg for k in aggs.keys() for agg in aggs[k]\n",
    "    ]\n",
    "    agg_train.columns = new_columns\n",
    "\n",
    "    agg_train['mjd_diff'+str(pband)] = agg_train['mjd'+ str(pband)+'_max'] - agg_train['mjd'+ str(pband)+'_min']\n",
    "    agg_train['flux_diff'+str(pband)] = agg_train['flux'+ str(pband)+'_max'] - agg_train['flux'+ str(pband)+'_min']\n",
    "    agg_train['flux_dif2'+str(pband)] = (agg_train['flux'+ str(pband)+'_max'] - agg_train['flux'+ str(pband)+'_min']) / agg_train['flux'+ str(pband)+'_mean']\n",
    "    agg_train['flux_w_mean'+str(pband)] = agg_train['flux_by_flux_ratio_sq' +str(pband)+'_sum'] / agg_train['flux_ratio_sq' +str(pband)  + '_sum']\n",
    "    agg_train['flux_dif3'+str(pband)] = (agg_train['flux'+str(pband) + '_max'] - agg_train['flux'+ str(pband) +'_min']) / agg_train['flux_w_mean'+str(pband)]\n",
    "\n",
    "    del agg_train['mjd'+str(pband)  +'_max'], agg_train['mjd'+str(pband)  +'_min']\n",
    "    #agg_train.head()\n",
    "    \n",
    "   # del agg_train['passband']\n",
    "    \n",
    "    agg_db = pd.merge(left=agg_db,right=agg_train,on='object_id')\n",
    "    \n",
    "    #result = pd.concat([df1, df4], axis=1, sort=False)\n",
    "    \n",
    "del train\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7ed21e25fb5678c78bb19a8297bf83db00ccae01"
   },
   "source": [
    "# Merging extracted features with meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "696ee870837c23375bc7f0388de1b07510351123"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes :  [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n"
     ]
    }
   ],
   "source": [
    "meta_train = pd.read_csv('../data/training_set_metadata.csv')\n",
    "meta_train.head()\n",
    "\n",
    "\n",
    "\n",
    "full_train = agg_db.reset_index().merge(\n",
    "    right=meta_train,\n",
    "    how='outer',\n",
    "    on='object_id'\n",
    ")\n",
    "\n",
    "if 'target' in full_train:\n",
    "    y = full_train['target']\n",
    "    del full_train['target']\n",
    "classes = sorted(y.unique())\n",
    "\n",
    "# Taken from Giba's topic : https://www.kaggle.com/titericz\n",
    "# https://www.kaggle.com/c/PLAsTiCC-2018/discussion/67194\n",
    "# with Kyle Boone's post https://www.kaggle.com/kyleboone\n",
    "class_weight = {\n",
    "    c: 1 for c in classes\n",
    "}\n",
    "for c in [64, 15]:\n",
    "    class_weight[c] = 2\n",
    "\n",
    "print('Unique classes : ', classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "2b6f79b71d0e9b62266a52d9e7ac209c56e14aff",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 'object_id' in full_train:\n",
    "    oof_df = full_train[['object_id']]\n",
    "    del full_train['object_id'], full_train['distmod'], full_train['hostgal_specz']\n",
    "    del full_train['ra'], full_train['decl'], full_train['gal_l'],full_train['gal_b'],full_train['ddf']\n",
    "    \n",
    "train_mean = full_train.mean(axis=0)\n",
    "full_train.fillna(train_mean, inplace=True)\n",
    "\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "del full_train['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "810d842e768ea358fa042b89aee03664cc31e3de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7848, 141)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this model will have 7848 sources to train/test on, and using 141 unique features for each. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7aaadd7fb68f26be1aa78e0721332689365f2322"
   },
   "source": [
    "# Standard Scaling the input (imp.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "c1aaa4b97bec65047182097d08d33541aaf9a057",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_train_new = full_train.copy()\n",
    "ss = StandardScaler()\n",
    "full_train_ss = ss.fit_transform(full_train_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "92b0ae5a7f0b581b1f4e971a5c62eb97d2397455"
   },
   "source": [
    "# Deep Learning Begins..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "7b7424590dc985a79354838aa99ed5cb94ebc3f7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,BatchNormalization,Dropout\n",
    "from keras.callbacks import ReduceLROnPlateau,ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "59889ae9a9041a1c9efa8c42c59b75aa58249bf1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/PLAsTiCC-2018/discussion/69795\n",
    "def mywloss(y_true,y_pred):  \n",
    "    yc=tf.clip_by_value(y_pred,1e-15,1-1e-15)\n",
    "    loss=-(tf.reduce_mean(tf.reduce_mean(y_true*tf.log(yc),axis=0)/wtable))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "4c35b11a75cdc6aa926109ae876a1f2bb9f4078c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multi_weighted_logloss(y_ohe, y_p):\n",
    "    \"\"\"\n",
    "    @author olivier https://www.kaggle.com/ogrellier\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    \"\"\"\n",
    "    classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
    "    class_weight = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n",
    "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
    "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1-1e-15)\n",
    "    # Transform to log\n",
    "    y_p_log = np.log(y_p)\n",
    "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
    "    # Exclude class 99 for now, since there is no class99 in the training set \n",
    "    # we gave a special process for that class\n",
    "    y_log_ones = np.sum(y_ohe * y_p_log, axis=0)\n",
    "    # Get the number of positives for each class\n",
    "    nb_pos = y_ohe.sum(axis=0).astype(float)\n",
    "    # Weight average and divide by the number of positives\n",
    "    class_arr = np.array([class_weight[k] for k in sorted(class_weight.keys())])\n",
    "    y_w = y_log_ones * class_arr / nb_pos    \n",
    "    loss = - np.sum(y_w) / np.sum(class_arr)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6601f669629d8e1bf9182054206ef316341e0e3e"
   },
   "source": [
    "# Defining simple model in keras\n",
    "\n",
    "Note this is the same as the one used in the original kernel, I don't feel confident enough to change the # of neurons and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "4a411f6097378ab0c3199064478f4ed4e064b6fb",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "def build_model(dropout_rate=0.25,activation='relu'):\n",
    "    start_neurons = 512\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(start_neurons, input_dim=full_train_ss.shape[1], activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(start_neurons//2,activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(start_neurons//4,activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(start_neurons//8,activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate/2))\n",
    "    \n",
    "    model.add(Dense(len(classes), activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "9235d7edc1520d10784f0f14b88e35651d7c9d5b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_y = np.unique(y)\n",
    "class_map = dict()\n",
    "for i,val in enumerate(unique_y):\n",
    "    class_map[val] = i\n",
    "        \n",
    "y_map = np.zeros((y.shape[0],))\n",
    "y_map = np.array([class_map[val] for val in y])\n",
    "y_categorical = to_categorical(y_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4b915ef7ac26164d0b83e265a277ea50e8557eac"
   },
   "source": [
    "# Calculating the class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "fe95383168c5bd60e4349d6bce4999ffdff02471",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_count = Counter(y_map)\n",
    "wtable = np.zeros((len(unique_y),))\n",
    "for i in range(len(unique_y)):\n",
    "    wtable[i] = y_count[i]/y_map.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "449a0c6c0f3baa75ef4719ef8e76ac5fa62d094b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_loss_acc(history):\n",
    "    plt.plot(history.history['loss'][1:])\n",
    "    plt.plot(history.history['val_loss'][1:])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('val_loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train','Validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(history.history['acc'][1:])\n",
    "    plt.plot(history.history['val_acc'][1:])\n",
    "    plt.title('model Accuracy')\n",
    "    plt.ylabel('val_acc')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train','Validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6417136e31687df52d5d9f85a83df00aaa6cf2eb"
   },
   "outputs": [],
   "source": [
    "clfs = []\n",
    "oof_preds = np.zeros((len(full_train_ss), len(classes)))\n",
    "epochs = 123\n",
    "batch_size = 100\n",
    "for fold_, (trn_, val_) in enumerate(folds.split(y_map, y_map)):\n",
    "    checkPoint = ModelCheckpoint(\"./keras.model\",monitor='val_loss',mode = 'min', save_best_only=True, verbose=0)\n",
    "    x_train, y_train = full_train_ss[trn_], y_categorical[trn_]\n",
    "    x_valid, y_valid = full_train_ss[val_], y_categorical[val_]\n",
    "    \n",
    "    model = build_model(dropout_rate=0.5,activation='tanh')    \n",
    "    model.compile(loss=mywloss, optimizer='adam', metrics=['accuracy'])\n",
    "    history = model.fit(x_train, y_train,\n",
    "                    validation_data=[x_valid, y_valid], \n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,shuffle=True,verbose=0,callbacks=[checkPoint])       \n",
    "    \n",
    "    plot_loss_acc(history)\n",
    "    \n",
    "    print('Loading Best Model')\n",
    "    model.load_weights('./keras.model')\n",
    "    # # Get predicted probabilities for each class\n",
    "    oof_preds[val_, :] = model.predict_proba(x_valid,batch_size=batch_size)\n",
    "    print(multi_weighted_logloss(y_valid, model.predict_proba(x_valid,batch_size=batch_size)))\n",
    "    clfs.append(model)\n",
    "    \n",
    "print('MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(y_categorical,oof_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d8d149b6795271bdce59ee091118bd1681cb3462",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cc5c23f2780db663e813e796b811f3831ef42686",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_map, np.argmax(oof_preds,axis=-1))\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "161c3e5e83a2ee3927df2c782fcb53fb7a379292"
   },
   "outputs": [],
   "source": [
    "sample_sub = pd.read_csv('../input/sample_submission.csv')\n",
    "class_names = list(sample_sub.columns[1:-1])\n",
    "del sample_sub;gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f9416f8a1b8b43e49f54642e03793ffce83adedd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(12,12))\n",
    "foo = plot_confusion_matrix(cnf_matrix, classes=class_names,normalize=True,\n",
    "                      title='Confusion matrix')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f65492856178f1692de22989481d7dde8afd1e84"
   },
   "source": [
    "# Test Set Predictions\n",
    "\n",
    "This will not be done here!! Too much for my computer to handle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2922fdae392d8b9e71882e809b479311e6db1844",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "meta_test = pd.read_csv('../input/test_set_metadata.csv')\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "chunks = 5000000\n",
    "for i_c, df in enumerate(pd.read_csv('../input/test_set.csv', chunksize=chunks, iterator=True)):\n",
    "    df['flux_ratio_sq'] = np.power(df['flux'] / df['flux_err'], 2.0)\n",
    "    df['flux_by_flux_ratio_sq'] = df['flux'] * df['flux_ratio_sq']\n",
    "    agg_db = pd.DataFrame([])#,columns='object_id')\n",
    "    agg_db['object_id'] = df['object_id'].unique()\n",
    "\n",
    "#agg_db = pd.DataFrame(data=agg_db['object_id'].unique(),columns='object_id')\n",
    "    for pband in df['passband'].unique():\n",
    "      #  print(\"Looking at passband \" + str(pband))\n",
    "        pband_data = df.loc[df['passband'] == pband]\n",
    "        for col in pband_data.columns:\n",
    "            if col != 'object_id':\n",
    "                pband_data[col + str(pband)] = pband_data[col]\n",
    "                del pband_data[col]\n",
    "    #pband_data['passband # '+ str(pband)] = pband_data['passband']\n",
    "   # del pband_data['passband']\n",
    "\n",
    "        aggs = {\n",
    "            'mjd'+ str(pband): ['min', 'max', 'size'],\n",
    "            #'passband' + str(pband): ['min', 'max', 'mean', 'median', 'std'],\n",
    "            'flux'+ str(pband): ['min', 'max', 'mean', 'median', 'std','skew'],\n",
    "            'flux_err'+ str(pband): ['min', 'max', 'mean', 'median', 'std','skew'],\n",
    "            'detected'+ str(pband): ['mean'],\n",
    "            'flux_ratio_sq'+ str(pband):['sum','skew'],\n",
    "            'flux_by_flux_ratio_sq'+ str(pband):['sum','skew'],\n",
    "        }\n",
    "\n",
    "\n",
    "        agg_test = pband_data.groupby('object_id').agg(aggs)\n",
    "        new_columns = [\n",
    "            k + '_' + agg for k in aggs.keys() for agg in aggs[k]\n",
    "        ]\n",
    "        agg_test.columns = new_columns\n",
    "\n",
    "        agg_test['mjd_diff'+str(pband)] = agg_test['mjd'+ str(pband)+'_max'] - agg_test['mjd'+ str(pband)+'_min']\n",
    "        agg_test['flux_diff'+str(pband)] = agg_test['flux'+ str(pband)+'_max'] - agg_test['flux'+ str(pband)+'_min']\n",
    "        agg_test['flux_dif2'+str(pband)] = (agg_test['flux'+ str(pband)+'_max'] - agg_test['flux'+ str(pband)+'_min']) / agg_test['flux'+ str(pband)+'_mean']\n",
    "        agg_test['flux_w_mean'+str(pband)] = agg_test['flux_by_flux_ratio_sq' +str(pband)+'_sum'] / agg_test['flux_ratio_sq' +str(pband)  + '_sum']\n",
    "        agg_test['flux_dif3'+str(pband)] = (agg_test['flux'+str(pband) + '_max'] - agg_test['flux'+ str(pband) +'_min']) / agg_test['flux_w_mean'+str(pband)]\n",
    "\n",
    "        del agg_test['mjd'+str(pband)  +'_max'], agg_test['mjd'+str(pband)  +'_min']\n",
    "#     del df\n",
    "#     gc.collect()\n",
    "    \n",
    "        agg_db = pd.merge(left=agg_db,right=agg_test,on='object_id')\n",
    "\n",
    "    \n",
    "    # Merge with meta data\n",
    "    full_test = agg_db.reset_index().merge(\n",
    "        right=meta_test,\n",
    "        how='left',\n",
    "        on='object_id'\n",
    "    )\n",
    "    full_test[full_train.columns] = full_test[full_train.columns].fillna(train_mean)\n",
    "    full_test_ss = ss.transform(full_test[full_train.columns])\n",
    "    # Make predictions\n",
    "    preds = None\n",
    "    for clf in clfs:\n",
    "        if preds is None:\n",
    "            preds = clf.predict_proba(full_test_ss) / folds.n_splits\n",
    "        else:\n",
    "            preds += clf.predict_proba(full_test_ss) / folds.n_splits\n",
    "    \n",
    "   # Compute preds_99 as the proba of class not being any of the others\n",
    "    # preds_99 = 0.1 gives 1.769\n",
    "    preds_99 = np.ones(preds.shape[0])\n",
    "    for i in range(preds.shape[1]):\n",
    "        preds_99 *= (1 - preds[:, i])\n",
    "    \n",
    "    # Store predictions\n",
    "    preds_df = pd.DataFrame(preds, columns=class_names)\n",
    "    preds_df['object_id'] = full_test['object_id']\n",
    "    preds_df['class_99'] = 0.14 * preds_99 / np.mean(preds_99) \n",
    "    \n",
    "    if i_c == 0:\n",
    "        preds_df.to_csv('predictions.csv',  header=True, mode='a', index=False)\n",
    "    else: \n",
    "        preds_df.to_csv('predictions.csv',  header=False, mode='a', index=False)\n",
    "        \n",
    "    del agg_test, full_test, preds_df, preds\n",
    "#     print('done')\n",
    "    if (i_c + 1) % 10 == 0:\n",
    "        print('%15d done in %5.1f' % (chunks * (i_c + 1), (time.time() - start) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ab549b5d9fc196d47d2a5d7b73f241dda07cdd8c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z = pd.read_csv('predictions.csv')\n",
    "\n",
    "print(z.groupby('object_id').size().max())\n",
    "print((z.groupby('object_id').size() > 1).sum())\n",
    "\n",
    "z = z.groupby('object_id').mean()\n",
    "\n",
    "z.to_csv('single_predictions.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e99166b5c98de7f41722d6319dd121d445828ba3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9e58e8719ced4eb61b9af02838e91340799cff37",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
